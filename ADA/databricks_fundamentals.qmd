---
title: "DataBricks fundamentals"
---

------------------------------------------------------------------------

# What is DataBricks?

------------------------------------------------------------------------

DataBricks is a web based platform for large scale data manipulation and analysis using code to create reproducible data pipelines. Primarily it takes the form of a website which you can create data pipelines and perform analysis in. It currently supports the languages R, SQL, python and scala, and integrates well with git based version control systems such as github or Azure DevOps.

Behind the scenes it is distributed cloud computing platform which utilizes the [Apache Spark engine](https://spark.apache.org/) to split up heavy data processing into smaller chunks. It then distributes them to different 'computers' within the cloud to perform the processing of each chunk in parallel. Once each 'computer' is finished processing the results are recombined and passed back to the user or stored.

Due to the parallel processing capabilities this improves the performance of the data processing and allows for the manipulation of very large data sets in a relatively short amount of time.

In addition, it also provides new tools within the platform to construct and automate complex data transformations and processing.

------------------------------------------------------------------------

## Key differences

------------------------------------------------------------------------

Underpinning the technology are some key differences in how computers we're familiar with, and DataBricks (and distributed computing in general) are structured.

### Traditional computing

Currently, we are used to using a PC or laptop to do our data processing. A traditional computer has all of the components it needs to function:

-   A processor and memory to do calculations
-   A hard drive to store data permanently on
-   A keyboard and mouse to capture user input
-   A screen to provide outputs to the user

![](/images/ada-traditional-computer.jpg){width="273"}

Any computer traditional computer is limited by it's hardware meaning there is an upper limit on the size and complexity of data it can process.

In order to increase the amount of data a computer can process you would have to switch out the physical hardware of the machine for something more powerful.

### On DataBricks

In DataBricks you can scale the components of your machine up (CPU cores, RAM) without having to build a physical machine to house them, essentially temporarily 'borrowing' processor power, memory and storage from a super computer.

This means you can perform very heavy analyses that your laptop wouldn't be able to cope with. The DataBricks platform takes the place of the keyboard/mouse and screen, taking input and providing output back to the user.

The storage and computation are separated into different components rather than being within the same 'machine'. Processing (processor and memory) is handled by a 'compute' resource, and storage (hard drive) is centralised in the 'unity catalog'.

![](/images/ada-cloud-computing.jpg){width="515"}

### Benefits of cloud compute

-   **Scalable** - if you need more computing power you can increase your computing power and only pay for what you use rather than having to build an expensive new machine
-   **Centralised** - All data, scripts, and processes are available in a single place and access for any other user can be controlled as required.
-   **Data Governance** - The Department is able to 'see' all of it's data and organisational knowledge. This enables to ensure it is access controlled and align with GDPR and data protection legislation and guidelines.
-   **Auditing and version control** - The Platform itself generates a lot of metadata which enables it to keep versioned history of it's data, outputs, etc.

------------------------------------------------------------------------

# Key concepts

------------------------------------------------------------------------

## Storage

------------------------------------------------------------------------

There are a few different ways of storing files and data on DataBricks. Your data, and modelling areas will reside in the 'unity catalog', whereas your scripts and code will live on your 'workspace'.

### Unity catalog

The majority of data and files on DataBricks should be stored in the 'unity catalog'. This is similar in concept to a traditional database server, however the unity catalog also contains file storage in the form of volumes.

The unity catalog can be accessed through the 'Catalog' option in the DataBricks sidebar.

![](/images/ada-unity-catalog-sidebar.png)

#### Catalogs not databases

The unity catalog contains numerous catalogs, which are similar in concept to a SQL database.

#### Schemas, tables and views

Like a SQL database a catalog has schemas, tables, and views which store data in a structured (usually tabular) format.

This is where you would store your core datasets and pick up data to analyse from.

#### Volumes

Unlike a SQL database the unity catalog also contains volumes, which are file storage similar to a shared drive. They can be used for storing any type of file.

Volumes are stored under a schema within a catalog. Files in here can be accessed and manipulated through code. You can also upload files to a volume through the user interface.

#### Structure of the unity catalog

There is one unity catalog which can contain any number of catalogs.

A catalog can contain any number of schemas.

A schema can contain any number of tables, views and volumes.

### ![](/images/ada-unity-catalog.jpg)

### Workspaces - DataBricks file system (DBFS)

Each user has their own workspace which serves as a personal document and code storage. It contains a user folder which is specific to each person, and any git repositories that you have cloned or created within DataBricks.

Your workspace can be accessed through the 'Catalog' option in the DataBricks sidebar.

![](/images/ada-workspace-repos.png){fig-align="center"}

Everything in your workspace is only accessible to you unless you share it with other users. When you do share a script or file you can specify whether the person you're sharing it with is able to view/edit/run the file your sharing.

::: callout-caution
## Don't overshare

Sharing code this way can be useful but has it's risks. If you allow other users to edit and run your workbooks it's possible that they can make changes or run it simultaneously resulting in unexpected results.

For collaboration on code it is always preferable to use a github/DevOps repository which each user can clone and work on independently.
:::

### Repositories for version control

Ideally all code should be managed through a versioned repository on github or Azure DevOps.

To connect databricks to a repository:

1.  Create a repository on your chosen platform (github / DevOps) and copy the HTTPS link to the repo.\
    \
    ![](/images/ada-workspace-repos-link.png)
2.  On the 'Workspace' page navigate to your user or repos folder then click the 'Create' button, then select 'Git folder'\
    \
    ![](/images/ada-repos-create.png)
3.  In the resulting window, paste your repo URL, select your git provider (here: DevOps) and given the git folder a name.\
    \
    ![](/images/ada-repos-create-details.png)
4.  This will create your git folder in your user folder which you can then add scripts, queries and notebooks to.\
    \
    ![](/images/ada-repos-git-folder.png)
5.  To commit and push your changes to the repository, or create/swap branches click on the git button to the right of the folder/notebook/script title. This will contain the name of the git branch that is current checked out which at first will be 'Main'.\
    ![](/images/ada-repos-git-button.png)
6.  This will bring up a commit window which will show you the differences to your file(s) since the last commit. Here you create or change branches, or can commit and push by writing a commit message and clicking the 'Commit and push' button.\
    \
    ![](/images/ada-repos-git-commit.png)

------------------------------------------------------------------------

## Compute

------------------------------------------------------------------------

In order to access data and run code you need to set up a compute resource. The compute page can be accessed through the 'Compute' option in the DataBricks sidebar.

![](/images/ada-compute.png)

There are several types of compute available and you will need to make the most appropriate choice for the kind of processing you're wanting to do. The types are:

-   SQL Warehouse - multiple users, SQL only

-   Personal cluster - single user, supports R, SQL, python and scala

-   Shared cluster - multiple users, supports SQL, python and scala

You are able to create a personal cluster yourself, whereas shared clusters and SQL warehouses have to be requested through the ADA team.

In most cases a personal cluster will be the most versatile and easily accessible option. However your team may want to request a shared SQL warehouse if you have a lot of processing heavy SQL queries.

All compute options can be used both within the DataBricks platform and be connected to through other applications such as R & RStudio.

### Creating a personal compute resource

1.  To create your own personal compute resource click the 'Create with DfE Personal Compute' button on the compute page.

![](/images/ada-compute-personal.png)

2.  You'll then be presented with a screen to configure the cluster. There are 2 options here under the performance section which you will want to pay attention to; Databricks runtime version, and Node type.\
    \
    **DataBricks runtime version** - This is the version of the DataBricks software that will be present on your node. Generally it is recommended you go with the latest LTS (long term support) version. At the time of writing this is '15.4 LTS'.\
    \
    **Node type** - This option determines how powerful your cluster is and there are 2 options available by default:\
    -   Standard 14GB 4-Core Nodes\
    -   Large 28GB 8-Core Nodes\
        \
        If you require a larger personal cluster this can be requested by the ADA team.\
        \
        ![](/images/ada-compute-personal-create.png)
3.  Click the 'Create compute' button at the bottom of the page. This will create your personal cluster and begin starting it up. This usually takes around 5 minutes.\
    \
    ![](/images/ada-compute-personal-create-button.png)

::: callout-important
As mentioned earlier compute resources have no storage of their own. This means that if you install libraries or packages onto a cluster they will only remain installed until the cluster is stopped. Once re-started those libraries will need to be installed again.

An alternative to this is to specify packages/libraries to be installed on the cluster at start up. To do this click the name of your cluster from the 'Compute' page, then go to the 'Libraries' tab and click the 'Install new' button.
:::

Once you have a compute resource you can begin using DataBricks. You can do this either through connecting to DataBricks through RStudio, or you can begin coding in the DataBricks platforms using scripts, or Notebooks (see below).

------------------------------------------------------------------------

## Notebooks

------------------------------------------------------------------------

Notebooks are a special kind of script that DataBricks supports. They consist of code blocks and markdown blocks which can contain formatted text, links and images.

You can create a notebook in your workspace, either in a folder or a repository.\
To do this locate the folder/repository you want to create the notebook in then click the 'Create' button and select Notebook.

::: callout-tip
It's generally recommended that any notebooks used for core business processes are created in a repository linked to github/DevOps where they can be version controlled.
:::

Once you've created a notebook it will automatically open it.

You can change the title from 'Untitled Notebook *\<timestamp\>*', and set it's default language in the dropdown immediately to the right of the notebook title.

![](/images/ada-notebook.png)

The default language is the language the notebook will assume all code chunks are written in. In the screenshot above the default language is 'R', so all chunks will be assumed to be written in R unless otherwise specified.

You can also add markdown cells to add text, links and graphics to you notebook in order to document the processing done within it.

To add a new code of markdown chunk move the mouse above or below another chunk and the buttons '+Code' and '+Text' will appear.

![](/images/ada-notebook-add-chunk.png)

To run code chunks you'll first need to attach your compute resource to it by clicking the 'Connect' button in the top right hand side of the page.

![](/images/ada-notebook-attach.png)

You can run a code chunk either by pressing the play button in the top left corner of the chunk, or by pressing Ctrl + Return/Enter on the keyboard. Any outputs that result from the code will be displayed underneath the chunk.

![](/images/ada-notebook-chunk-output.png)

Everything ran in a notebook is in it's own 'session' meaning that later chunks have access to variables, functions, etc. that were defined above.

Notebooks cannot share a session with another Notebook so bear this in mind when constructing your workflows. If you need to pass data between notebooks it can be written out to a table in the unity catalog and accessed through that route.

Notebooks can be parameterised using '[widgets](https://docs.databricks.com/en/notebooks/widgets.html)', meaning a single notebook can be re-used with different inputs. This means they can be used in a similar way to a function in R/python or a stored procedure in SQL.

------------------------------------------------------------------------

## Workflows

------------------------------------------------------------------------

Workflows allow you to build complex data pipelines by chaining together multiple scripts, queries, notebooks and logic. Each step in a workflow is referred to as a task and each task has dependencies. They are accessible through the 'Workflows' link on the left hand menu of the DataBricks UI.

![](/images/ada-workflow-menu.png)

Parameters can be set either at a workflow level or a task level, allowing you to reuse tasks/workflows for similar operations.

Each task can have dependencies on other tasks and can be set to only run under certain conditions, for example all of the previous tasks have completed successfully.

Workflows can also be configured to send notifications to users upon success/failure.

You can create a new workflow from the Workflow page by clicking the 'Create job' button in the top right of the screen.

After setting up a flow of tasks you will be presented with a graphical presentation of the workflow as seen below:

![](/images/ada-workflow-task-chart.png)

Each time a workflow is ran DataBricks audits any input parameters, all outputs, the success and failure of each task, along with when it was run and who by.

This makes them a very powerful debugging tool as you can refer back to results from previous runs.

![](/images/ada-workflow-task-output.png)

Another useful aspect of workflows is that they can be defined and ran using code through the [DataBricks Jobs API](https://docs.databricks.com/api/workspace/jobs) by passing tasks as JSON lists. Alternatively they can be created using only the R language using the [DataBricks SDK for R package](https://docs.databricks.com/en/dev-tools/sdk-r.html) using lists instead of JSON.

Workflows also come with robust support for git/DevOps repositories and can be set to run from a specific repo, branch, commit or tag.
